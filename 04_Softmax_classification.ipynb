{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devyulbae/AIClass/blob/main/04_Softmax_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JutkdoS4X-A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4d4abb78-fa3b-4b99-aee3-548ac75ceea4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.15.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8vMWSpBX3iX"
      },
      "source": [
        "# Softmax classification\n",
        "\n",
        "* 임의의 Dataset 준비\n",
        "* 3개의 클래스로 분류할 데이터 준비\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZH1QvjlFBsE"
      },
      "source": [
        "x_data = [[1., 2., 1., 1.],\n",
        "          [2., 1., 3., 2.],\n",
        "          [3., 1., 3., 4.],\n",
        "          [4., 1., 5., 5.],\n",
        "          [1., 7., 5., 5.],\n",
        "          [1., 2., 5., 6.],\n",
        "          [1., 6., 6., 6.],\n",
        "          [1., 7., 7., 7.]] # 8x4\n",
        "y_data = [[0., 0., 1.],\n",
        "          [0., 0., 1.],\n",
        "          [0., 0., 1.],\n",
        "          [0., 1., 0.],\n",
        "          [0., 1., 0.],\n",
        "          [0., 1., 0.],\n",
        "          [1., 0., 0.],\n",
        "          [1., 0., 0.]] # 8x3\n",
        "\n",
        "x_test = [[1., 1., 1., 1.]]\n",
        "y_test = [[0., 0., 1.]]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCPXA2luyvlN"
      },
      "source": [
        "## 임의의 Data를 이용해서 3개의 클래스를 가지는 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpNjJzwwX7uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50b9c689-f325-4253-a4af-fd3149d30dd4"
      },
      "source": [
        "#dataset을 선언합니다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data))\n",
        "dataset = dataset.repeat(1).batch(8)\n",
        "\n",
        "nb_classes = 3 # class의 개수입니다.\n",
        "\n",
        "print(tf.Variable(x_data))\n",
        "print(tf.Variable(y_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(8, 4) dtype=float32, numpy=\n",
            "array([[1., 2., 1., 1.],\n",
            "       [2., 1., 3., 2.],\n",
            "       [3., 1., 3., 4.],\n",
            "       [4., 1., 5., 5.],\n",
            "       [1., 7., 5., 5.],\n",
            "       [1., 2., 5., 6.],\n",
            "       [1., 6., 6., 6.],\n",
            "       [1., 7., 7., 7.]], dtype=float32)>\n",
            "<tf.Variable 'Variable:0' shape=(8, 3) dtype=float32, numpy=\n",
            "array([[0., 0., 1.],\n",
            "       [0., 0., 1.],\n",
            "       [0., 0., 1.],\n",
            "       [0., 1., 0.],\n",
            "       [0., 1., 0.],\n",
            "       [0., 1., 0.],\n",
            "       [1., 0., 0.],\n",
            "       [1., 0., 0.]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmHTiEKcX_zD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f57cbce-27ac-4d29-922c-2d7509ec2eeb"
      },
      "source": [
        "#Weight and bias setting\n",
        "W = tf.Variable(tf.random.normal([4, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([nb_classes]), name='bias')\n",
        "variables = [W, b]\n",
        "\n",
        "tf.print(W,b)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.101562597 -1.23245883 -2.38226604]\n",
            " [0.444608241 0.649644434 -0.722008765]\n",
            " [1.50522554 -1.18143976 -1.22825968]\n",
            " [-0.44749102 -0.164771929 1.02197492]] [-0.427955598 1.42194891 -1.01549339]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4op7dp4RyvlT"
      },
      "source": [
        "# 가설 설정\n",
        "\n",
        "* 가설에서 예측한 값들을 이용해 예측값들을 확률로 표현한다.\n",
        "\n",
        "## $$ y_k = \\frac{exp(x_k)}{\\sum_{i=1}^{n}exp(x_i)}  $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kje8MUl-DOMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eed79ae-ac41-479e-f00b-c0f379fe351a"
      },
      "source": [
        "# tf.nn.softmax computes softmax activations\n",
        "# softmax = exp(logits) / reduce_sum(exp(logits), dim)\n",
        "def hypothesis_softmax(X):\n",
        "    return tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "tf.print(hypothesis_softmax(tf.Variable(x_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.813427567 0.185539216 0.0010331762]\n",
            " [0.999692619 0.000301130262 6.24554059e-06]\n",
            " [0.999850512 0.000139646945 9.84662256e-06]\n",
            " ...\n",
            " [0.999944627 2.01740895e-05 3.51710878e-05]\n",
            " [0.999996841 3.12030693e-06 2.15005169e-08]\n",
            " [0.999999583 3.46117304e-07 1.89164373e-09]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYnGTBv2S-DS"
      },
      "source": [
        "## 가설을 검증할 Cross Entropy 함수를 정의합니다\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "cost(h(x),y) & = −\\sum_{n=1}^{n} Y log(h(x))\n",
        "\\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxXa5whUIwSN"
      },
      "source": [
        "def loss_fn(hypothesis, labels):\n",
        "    loss = tf.keras.losses.categorical_crossentropy(labels, hypothesis)\n",
        "    # tf.keras.losses.binary_crossentropy(labels, hypothesis) # 이진분류용 CE Loss\n",
        "    return loss\n",
        "\n",
        "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=0.01)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-3BTbBLyvla"
      },
      "source": [
        "### 학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHOKhWyIzk9",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2dd0f9c-7a8f-4838-ff3f-9a3cf52e26cd"
      },
      "source": [
        "epochs = 5000\n",
        "\n",
        "for step in range(epochs):\n",
        "  for features, labels in dataset:\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = loss_fn(hypothesis_softmax(features),labels)\n",
        "      grads = tape.gradient(loss_value, [W,b])\n",
        "      optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b]))\n",
        "      if step % 100 == 0:\n",
        "            print(\"Iter: {}, Loss: {:.4f}\".format(step, tf.reduce_mean(loss_value.numpy()).numpy()))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter: 0, Loss: 8.3210\n",
            "Iter: 100, Loss: 0.6846\n",
            "Iter: 200, Loss: 0.4935\n",
            "Iter: 300, Loss: 0.4190\n",
            "Iter: 400, Loss: 0.3760\n",
            "Iter: 500, Loss: 0.3458\n",
            "Iter: 600, Loss: 0.3223\n",
            "Iter: 700, Loss: 0.3029\n",
            "Iter: 800, Loss: 0.2862\n",
            "Iter: 900, Loss: 0.2717\n",
            "Iter: 1000, Loss: 0.2587\n",
            "Iter: 1100, Loss: 0.2470\n",
            "Iter: 1200, Loss: 0.2364\n",
            "Iter: 1300, Loss: 0.2266\n",
            "Iter: 1400, Loss: 0.2177\n",
            "Iter: 1500, Loss: 0.2095\n",
            "Iter: 1600, Loss: 0.2018\n",
            "Iter: 1700, Loss: 0.1947\n",
            "Iter: 1800, Loss: 0.1881\n",
            "Iter: 1900, Loss: 0.1819\n",
            "Iter: 2000, Loss: 0.1761\n",
            "Iter: 2100, Loss: 0.1706\n",
            "Iter: 2200, Loss: 0.1655\n",
            "Iter: 2300, Loss: 0.1607\n",
            "Iter: 2400, Loss: 0.1561\n",
            "Iter: 2500, Loss: 0.1518\n",
            "Iter: 2600, Loss: 0.1477\n",
            "Iter: 2700, Loss: 0.1438\n",
            "Iter: 2800, Loss: 0.1401\n",
            "Iter: 2900, Loss: 0.1366\n",
            "Iter: 3000, Loss: 0.1332\n",
            "Iter: 3100, Loss: 0.1300\n",
            "Iter: 3200, Loss: 0.1270\n",
            "Iter: 3300, Loss: 0.1241\n",
            "Iter: 3400, Loss: 0.1213\n",
            "Iter: 3500, Loss: 0.1186\n",
            "Iter: 3600, Loss: 0.1161\n",
            "Iter: 3700, Loss: 0.1136\n",
            "Iter: 3800, Loss: 0.1113\n",
            "Iter: 3900, Loss: 0.1090\n",
            "Iter: 4000, Loss: 0.1069\n",
            "Iter: 4100, Loss: 0.1048\n",
            "Iter: 4200, Loss: 0.1028\n",
            "Iter: 4300, Loss: 0.1009\n",
            "Iter: 4400, Loss: 0.0990\n",
            "Iter: 4500, Loss: 0.0972\n",
            "Iter: 4600, Loss: 0.0955\n",
            "Iter: 4700, Loss: 0.0938\n",
            "Iter: 4800, Loss: 0.0922\n",
            "Iter: 4900, Loss: 0.0906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6iMuYpxyvld"
      },
      "source": [
        "## Sample 데이터를 넣고 테스트해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyNxH16II0st",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c44cd6-237b-401b-9987-d7c9908c5397"
      },
      "source": [
        "sample_data = [[2,1,3,2]] # answer_label [[0,0,1]]\n",
        "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
        "\n",
        "a = hypothesis_softmax(sample_data)\n",
        "\n",
        "print(a)\n",
        "print(tf.argmax(a, 1)) #index: 2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.00112876 0.04634704 0.9525242 ]], shape=(1, 3), dtype=float32)\n",
            "tf.Tensor([2], shape=(1,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rB-aJf0yvlg"
      },
      "source": [
        "## 데이터를 이용해서 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qJfHxEnI25d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7891059-3e28-431c-b4a4-b3f451fe3a89"
      },
      "source": [
        "b = hypothesis_softmax(x_test)\n",
        "print(b)\n",
        "print(tf.argmax(b, 1))\n",
        "print(tf.argmax(y_test, 1)) # matches with y_data\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[2.8657442e-07 4.7645762e-04 9.9952328e-01]], shape=(1, 3), dtype=float32)\n",
            "tf.Tensor([2], shape=(1,), dtype=int64)\n",
            "tf.Tensor([2], shape=(1,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkhpaF-rCMnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29825cf-93d6-498d-f4be-92591a08b3a4"
      },
      "source": [
        "def accuracy_fn(hypothesis, labels):\n",
        "    hypothesis = tf.argmax(hypothesis)\n",
        "    hypothesis = tf.cast(hypothesis, dtype=tf.float32)\n",
        "    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "    labels = tf.argmax(labels)\n",
        "    labels = tf.cast(labels, dtype=tf.float32)\n",
        "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.float32))\n",
        "    return accuracy\n",
        "\n",
        "test_acc = accuracy_fn(hypothesis_softmax(x_test),y_test)\n",
        "print(\"Testset Accuracy: {:.4f}\".format(test_acc))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testset Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHq7WwDzE1oh"
      },
      "source": [],
      "execution_count": 14,
      "outputs": []
    }
  ]
}