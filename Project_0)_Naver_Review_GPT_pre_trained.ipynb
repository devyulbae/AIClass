{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devyulbae/AIClass/blob/main/Project_0)_Naver_Review_GPT_pre_trained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oq4UnWajC9u"
      },
      "source": [
        "# Sentiment Classification\n",
        "\n",
        "### Task\n",
        "* 네이버에서 영화평을 가지고 positive/negative인지 구분해보자.\n",
        "* 데이터 불러오기를 제외한 딥러닝 트레이닝 과정을 직접 구현해보는 것이 목표 입니다.\n",
        "\n",
        "### Dataset\n",
        "* [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc/)\n",
        "\n",
        "### Base code\n",
        "* Dataset: train, val, test로 split\n",
        "* Input data shape: (`batch_size`, `max_sequence_length`)\n",
        "* Output data shape: (`batch_size`, 1)\n",
        "* Training\n",
        "* Evaluation\n",
        "\n",
        "### Try some techniques\n",
        "* Training-epochs 조절\n",
        "* Change model architectures (Custom model)\n",
        "  * Use another cells (LSTM, GRU, etc.)\n",
        "  * Use dropout layers\n",
        "* Embedding size 조절\n",
        "  * 또는 one-hot vector로 학습\n",
        "* Number of words in the vocabulary 변화\n",
        "* `pad` 옵션 변화\n",
        "* Data augmentation (if possible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW9rRFr4jC9w"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibO_zjJmvN3s"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgCS8iATktuT"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct7ZVZ2EjC9x"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import clear_output\n",
        "import urllib.request\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "from collections import Counter, defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e3ucn5_jC90"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "* ratings_train.txt: 훈련용으로 사용되는 15만 개의 리뷰\n",
        "* ratings_test.txt: 테스트용으로 보류된 5만 개의 리뷰\n",
        "* 모든 리뷰는 140자 이내입니다\n",
        "* 각 감정 클래스는 동등하게 샘플링되었습니다 (즉, 무작위 추측은 50%의 정확도를 보입니다)\n",
        "* 10만 개의 부정적 리뷰 (원래 1-4점의 리뷰)\n",
        "* 10만 개의 긍정적 리뷰 (원래 9-10점의 리뷰)\n",
        "* 중립적 리뷰 (원래 5-8점의 리뷰)는 제외되었습니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBqk_wq-RIYp"
      },
      "outputs": [],
      "source": [
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pB4ds-e1RavF"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_table('ratings_train.txt')\n",
        "train_data = train_data.dropna()\n",
        "test_data = pd.read_table('ratings_test.txt')\n",
        "test_data = test_data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdMHf_rOSGXS"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGCz3J5N3JeT"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq0jwEhcSThR"
      },
      "source": [
        "### Tokenizing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEfUnWZyT8px"
      },
      "outputs": [],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('/content/drive/MyDrive/dataset/naver_review/naver_review.model')  # 모델 경로 설정\n",
        "\n",
        "# 토크나이저 함수 정의\n",
        "def tokenizer(text):\n",
        "    return sp.encode_as_pieces(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPWz7QU_UFBK"
      },
      "outputs": [],
      "source": [
        "for i, (line) in enumerate(train_data['document']):\n",
        "    print(line)\n",
        "    print(sp.encode_as_pieces(line))\n",
        "    print(sp.encode_as_ids(line))\n",
        "    if i == 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHyID7qIRoWz"
      },
      "outputs": [],
      "source": [
        "eos_token = '[BOS]'\n",
        "eos_id = sp.piece_to_id(eos_token)\n",
        "\n",
        "print(f\"토큰 '{eos_token}'의 ID: {eos_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAB1othYFZi7"
      },
      "outputs": [],
      "source": [
        "sp.encode_as_ids(['[EOS]'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFV87kQjeEDC"
      },
      "outputs": [],
      "source": [
        "BOS_id = sp.piece_to_id('[BOS]')\n",
        "EOS_id = sp.piece_to_id('[EOS]')\n",
        "\n",
        "# 결과 출력\n",
        "lengths = []\n",
        "input_train_text, target_train_text = [], []\n",
        "for line in train_data['document']:\n",
        "    # Encode line, add BOS at the beginning and EOS at the end\n",
        "    input_line = #TODO\n",
        "    target_line = #TODO\n",
        "    input_train_text.append(tf.convert_to_tensor(input_line, dtype=tf.int32))\n",
        "    target_train_text.append(tf.convert_to_tensor(target_line, dtype=tf.int32))\n",
        "    lengths.append(len(line))\n",
        "\n",
        "input_test_text, target_test_text = [], []\n",
        "for line in test_data['document']:\n",
        "    # Similar process for test data\n",
        "    input_line = #TODO\n",
        "    target_line = #TODO\n",
        "    input_test_text.append(tf.convert_to_tensor(input_line, dtype=tf.int32))\n",
        "    target_test_text.append(tf.convert_to_tensor(target_line, dtype=tf.int32))\n",
        "    lengths.append(len(line))\n",
        "\n",
        "print(max(lengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4OeUYsx9OQ1"
      },
      "outputs": [],
      "source": [
        "print(len(input_test_text), len(target_train_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z1VU4WpGEXI"
      },
      "outputs": [],
      "source": [
        "print(input_test_text[0], target_train_text[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8yYy2OPjC-a"
      },
      "source": [
        "### Padding and truncating data using pad sequences\n",
        "* 전부 길이가 다른 리뷰들의 길이를 통일해주자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPV-UWrwQMvX"
      },
      "outputs": [],
      "source": [
        "batch_size = #TODO\n",
        "max_seq_length = #TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QB_qLXV3u_C"
      },
      "outputs": [],
      "source": [
        "input_train_data_pad = pad_sequences(#TODO)\n",
        "target_train_data_pad = pad_sequences(#TODO)\n",
        "input_test_data_pad = pad_sequences(#TODO)\n",
        "target_test_data_pad = pad_sequences(#TODO)\n",
        "\n",
        "\n",
        "print(input_train_data_pad.shape, target_train_data_pad.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CPqHeI9y7g2"
      },
      "source": [
        "### Dataset 구성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi43bh7G7NwG"
      },
      "outputs": [],
      "source": [
        "# for train\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((#TODO))\n",
        "train_dataset = train_dataset.shuffle(10000).repeat().batch(batch_size=#TODO)\n",
        "print(train_dataset)\n",
        "\n",
        "# for test\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((#TODO))\n",
        "test_dataset = test_dataset.batch(batch_size=#TODO)\n",
        "print(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMG2MOXUjC-y"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5KBM5sdjC-v"
      },
      "source": [
        "## Setup hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnhhKDlP8CqT"
      },
      "outputs": [],
      "source": [
        "kargs = {'model_name': 'GPT',\n",
        "         'num_layers': 12,\n",
        "         'd_model': 768,\n",
        "         'num_heads': 12,\n",
        "         'dff': 768 * 4,\n",
        "         'input_vocab_size': sp.get_piece_size(),\n",
        "         'target_vocab_size': sp.get_piece_size(),\n",
        "         'maximum_position_encoding': max_seq_length,\n",
        "         'segment_encoding': 2,\n",
        "         'end_token_idx': sp.piece_to_id('[EOS]'),\n",
        "         'rate': 0.1\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvflfLNJsmYU"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * i//2) / np.float32(d_model))\n",
        "    return pos * angle_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMSqmsT9ssT9"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5fQof6usttB"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "    output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWju3MTjsub-"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = kargs['num_heads']\n",
        "        self.d_model = kargs['d_model']\n",
        "\n",
        "        assert self.d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = self.d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wk = tf.keras.layers.Dense(kargs['d_model'])\n",
        "        self.wv = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(kargs['d_model'])\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10a_ZGMasyvr"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(**kargs):\n",
        "    return tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv1D(#TODO),  # (batch_size, seq_len, dff)\n",
        "            tf.keras.layers.Conv1D(#TODO)  # (batch_size, seq_len, d_model)\n",
        "        ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAeS5HH8Sb0L"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(**kargs)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(**kargs)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "        self.dropout3 = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "\n",
        "    def call(self, x, look_ahead_mask, padding_mask):\n",
        "        attn1, attn_weights_block1 = self.mha(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output)\n",
        "        out2 = self.layernorm3(ffn_output + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out2, attn_weights_block1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hh-S4n1tTMro"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kargs):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = kargs['d_model']\n",
        "        self.num_layers = kargs['num_layers']\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(kargs['target_vocab_size'], self.d_model)\n",
        "        self.pos_encoding = positional_encoding(kargs['maximum_position_encoding'], self.d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(**kargs)\n",
        "                           for _ in range(self.num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n",
        "\n",
        "    def call(self, x, look_ahead_mask, padding_mask):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1 = self.dec_layers[i](x, look_ahead_mask, padding_mask)\n",
        "\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRypbmACOrM8"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "        mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "        return mask  # (seq_len, seq_len)\n",
        "\n",
        "def create_masks(input):\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(input)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(input)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(input)\n",
        "    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return look_ahead_mask, dec_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ECeQU2bV_nc"
      },
      "outputs": [],
      "source": [
        "class GPT(tf.keras.Model):\n",
        "    def __init__(self, **kargs):\n",
        "        super(GPT, self).__init__(name=kargs['model_name'])\n",
        "        self.end_token_idx = kargs['end_token_idx']\n",
        "        self.decoder = Decoder(**kargs)\n",
        "        self.outputs_layer = tf.keras.layers.Dense(#TODO,\n",
        "                                                   activation=#TODO)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(#TODO)\n",
        "\n",
        "    def call(self, x):\n",
        "        look_ahead_mask, mask = create_masks(#TODO)\n",
        "\n",
        "        dec_output, attn = self.decoder(#TODO, #TODO, #TODO)  # (batch_size, inp_seq_len, d_model)\n",
        "        dec_output = self.outputs_layer(dec_output)  # (batch_size, inp_seq_len, d_model)\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_vocab_size)\n",
        "\n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX6TMRTnWfjt"
      },
      "outputs": [],
      "source": [
        "model = GPT(**kargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cruLtRx2jC_C"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qswjggp_i0r1"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "\n",
        "def loss(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "def accuracy(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
        "    pred *= mask\n",
        "    acc = train_accuracy(real, pred)\n",
        "\n",
        "    return tf.reduce_mean(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHJgMz0AitbW"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(#TODO),\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE8AQSGyW0b5"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                     monitor='val_loss',\n",
        "                                                     restore_best_weights=True,\n",
        "                                                     verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMhtscA7WoiK"
      },
      "outputs": [],
      "source": [
        "history = model.fit(#TODO)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWB5yVQonm1m"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-c8qXnK1nobL"
      },
      "outputs": [],
      "source": [
        "results = model.evaluate(test_dataset)\n",
        "# loss\n",
        "print(\"loss value: {:.3f}\".format(results[0]))\n",
        "# accuracy\n",
        "print(\"accuracy value: {:.3f}\".format(results[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-40F3YgugDwS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "position": {
        "height": "553px",
        "left": "792px",
        "right": "61px",
        "top": "71px",
        "width": "375px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}